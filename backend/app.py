# backend/app.py
from flask import Flask, request, jsonify
from flask_cors import CORS # Import CORS for frontend communication
import datetime
import os
import io
import json # For loading NDJSON files
import time
from pymongo.errors import BulkWriteError, ConnectionFailure
from pymongo import ReturnDocument # Needed for find_one_and_update to return the new document
import pymongo # Import pymongo specifically for UpdateOne in batch operations
import pandas as pd # Import pandas for CSV processing

# Import MongoDB client functions from db_client.py
from db_client import get_db, connect_to_mongodb, close_mongodb_connection

app = Flask(__name__)
CORS(app) # Enable CORS for all routes to allow frontend connection

# Paths to your pre-generated NDJSON files (created by data_prep.py)
PRODUCTS_JSON_PATH = 'products.json'
STORES_JSON_PATH = 'stores.json'
INVENTORY_JSON_PATH = 'inventory.json'

# --- Initial Data Loading Function (Programmatic Import to MongoDB) ---
def load_initial_inventory_data():
    """
    Loads initial data from generated NDJSON files into MongoDB.
    Includes aggressive rate limiting and basic retry logic.
    """
    try:
        db = get_db() # Ensure connection is established
    except Exception as e:
        print(f"Failed to connect to MongoDB, cannot load initial data: {e}")
        return

    # Check if all necessary JSON files exist
    if not (os.path.exists(PRODUCTS_JSON_PATH) and
            os.path.exists(STORES_JSON_PATH) and
            os.path.exists(INVENTORY_JSON_PATH)):
        print(f"Error: One or more NDJSON files not found.")
        print(f"Please ensure '{PRODUCTS_JSON_PATH}', '{STORES_JSON_PATH}', and '{INVENTORY_JSON_PATH}'")
        print("are in the same directory and were generated by data_prep.py.")
        return

    print("Starting initial data load from NDJSON files into MongoDB...")
    
    collections_to_load = {
        'products': PRODUCTS_JSON_PATH,
        'stores': STORES_JSON_PATH,
        'inventory': INVENTORY_JSON_PATH,
    }

    # Configuration for rate limiting and retries
    max_batch_size = 100 # Smaller batch size for initial load
    delay_between_batches = 3.0 # Seconds to wait between batch inserts (increased for safety)
    max_retries = 5
    retry_delay_multiplier = 2 # Multiplier for exponential backoff on retry

    print(f"Delay between batches set to: {delay_between_batches} seconds")
    print(f"Max batch size set to: {max_batch_size} operations")

    for collection_name, file_path in collections_to_load.items():
        print(f"\n--- Loading {collection_name} collection from {file_path} ---")
        
        items_to_insert = []
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip(): # Avoid empty lines
                    item = json.loads(line)
                    # Prepare data for MongoDB insertion
                    if collection_name == 'inventory':
                        # Ensure 'last_updated' is a datetime object
                        if 'last_updated' in item and isinstance(item['last_updated'], str):
                            try:
                                item['last_updated'] = datetime.datetime.fromisoformat(item['last_updated'])
                            except ValueError:
                                item['last_updated'] = datetime.datetime.now()
                        else:
                            item['last_updated'] = datetime.datetime.now()
                        # Ensure numerical fields are proper types
                        item['current_stock'] = int(item.get('current_stock', 0))
                        item['daily_sales_simulation_base'] = int(item.get('daily_sales_simulation_base', 1))

                    items_to_insert.append(item)
        
        print(f"Read {len(items_to_insert)} items for {collection_name}.")

        if not items_to_insert:
            print(f"No items to load for {collection_name}. Skipping.")
            continue

        current_collection = db[collection_name]
        
        # Drop the collection before inserting to ensure a clean slate on re-run (for development)
        # CAUTION: In production, you would NOT do this for existing data
        print(f"Dropping existing '{collection_name}' collection for clean import...")
        current_collection.drop()

        # Perform bulk inserts with rate limiting
        total_items_processed = 0
        for i in range(0, len(items_to_insert), max_batch_size):
            batch_data = items_to_insert[i:i + max_batch_size]
            
            retries = 0
            while retries < max_retries:
                try:
                    # Insert many documents at once
                    current_collection.insert_many(batch_data, ordered=False) # ordered=False continues on error
                    total_items_processed += len(batch_data)
                    print(f"  Inserted {len(batch_data)} {collection_name} records (Total: {total_items_processed}). Waiting {delay_between_batches}s...")
                    time.sleep(delay_between_batches)
                    break # Batch inserted successfully, break retry loop
                except BulkWriteError as bwe:
                    print(f"  BulkWriteError for {collection_name}: {bwe.details}")
                    # For a hackathon, we might just log and continue on BulkWriteError (e.g., duplicate key)
                    total_items_processed += bwe.details.get('nInserted', 0) # Count successful inserts in batch
                    print(f"  Partial batch inserted. Total: {total_items_processed}. Waiting {delay_between_batches}s...")
                    time.sleep(delay_between_batches)
                    break # Continue even on partial errors for now
                except ConnectionFailure as ce:
                    print(f"  MongoDB ConnectionFailure during insert for {collection_name}: {ce}")
                    retries += 1
                    current_retry_delay = delay_between_batches * (retry_delay_multiplier ** retries)
                    print(f"  Connection error. Retrying in {current_retry_delay}s... (Attempt {retries}/{max_retries})")
                    time.sleep(current_retry_delay)
                except Exception as e:
                    print(f"  Error inserting batch for {collection_name}: {e}")
                    retries += 1
                    current_retry_delay = delay_between_batches * (retry_delay_multiplier ** retries)
                    print(f"  General error. Retrying in {current_retry_delay}s... (Attempt {retries}/{max_retries})")
                    time.sleep(current_retry_delay)

            if retries == max_retries:
                print(f"  Failed to insert batch for {collection_name} after {max_retries} retries. Skipping remaining items in this batch.")

        # Ensure all items processed messages are printed for this collection even if batch_item_count is less than max_batch_size
        if total_items_processed < len(items_to_insert):
            print(f"  Note: Some items might have been skipped for {collection_name} due to errors.")
        
        print(f"Finished loading {collection_name}. Total {total_items_processed} items processed.")

    print("\n--- Initial data load process complete ---")
    print("You can now run 'flask run' to start the backend API.")

try:
    pass
except Exception as e:
    print(f"An unexpected error occurred during initial data loading: {e}")
    import traceback
    traceback.print_exc() # Print full traceback for debugging
finally:
    pass # The get_db() already handles connection management, no need to close here immediately


# --- API Endpoints ---

@app.route('/')
def home():
    """
    A simple home route to confirm the backend is running.
    """
    return "Walmart Inventory Management Backend is running! Access /inventory, /inventory/sale, /inventory/receipt, /inventory/low_stock_alerts."

@app.route('/inventory/<string:store_id>/<string:product_id>', methods=['GET'])
def get_inventory(store_id, product_id):
    """
    Retrieves the current stock level and details for a specific product
    at a given store.

    Example Request: GET /inventory/S1/P1
    Example Response:
    {
        "current_stock": 150,
        "last_updated": "2023-10-27 10:30:00",
        "product_id": "P1",
        "store_id": "S1"
    }
    """
    try:
        db = get_db()
    except Exception as e:
        return jsonify({"error": f"Database connection error: {e}"}), 500

    try:
        # MongoDB finds documents in the 'inventory' collection
        # We search by store_id and product_id fields.
        inventory_item = db.inventory.find_one({
            'store_id': store_id,
            'product_id': product_id
        })

        if inventory_item:
            # Convert MongoDB's default _id to string for JSON serialization
            inventory_item['_id'] = str(inventory_item['_id'])
            # Convert datetime object to string for JSON serialization
            if 'last_updated' in inventory_item and isinstance(inventory_item['last_updated'], datetime.datetime):
                inventory_item['last_updated'] = inventory_item['last_updated'].strftime('%Y-%m-%d %H:%M:%S')
            return jsonify(inventory_item), 200
        else:
            return jsonify({"message": f"Inventory not found for Product ID: {product_id} at Store ID: {store_id}"}), 404
    except Exception as e:
        print(f"Error fetching inventory: {e}")
        return jsonify({"error": f"An error occurred while fetching inventory: {str(e)}"}), 500

@app.route('/inventory/sale', methods=['POST'])
def record_sale():
    """
    Records a sale event, decrementing the inventory level for a product
    at a specific store.

    Example Request (JSON Body):
    {
        "store_id": "S1",
        "product_id": "P1",
        "quantity": 2
    }
    Example Response:
    {
        "message": "Sale recorded successfully",
        "new_stock_level": 148,
        "product_id": "P1",
        "quantity_sold": 2,
        "store_id": "S1"
    }
    """
    try:
        db = get_db()
    except Exception as e:
        return jsonify({"error": f"Database connection error: {e}"}), 500

    data = request.get_json()
    store_id = data.get('store_id')
    product_id = data.get('product_id')
    quantity = data.get('quantity')

    # Basic input validation
    if not all([store_id, product_id, quantity is not None]):
        return jsonify({"error": "Missing 'store_id', 'product_id', or 'quantity' in request body."}), 400
    if not isinstance(quantity, int) or quantity <= 0:
        return jsonify({"error": "Quantity must be a positive integer."}), 400

    try:
        # Find the document and atomically update its current_stock
        # Use $inc to decrement and $set to update last_updated
        # We also ensure stock is sufficient using $gte in the filter
        result = db.inventory.find_one_and_update(
            {'store_id': store_id, 'product_id': product_id, 'current_stock': {'$gte': quantity}},
            {'$inc': {'current_stock': -quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_sold_quantity': quantity}},
            return_document=ReturnDocument.AFTER # Return the updated document AFTER the update
        )

        if result:
            new_stock_level = result['current_stock']
            return jsonify({
                "message": "Sale recorded successfully",
                "product_id": product_id,
                "store_id": store_id,
                "quantity_sold": quantity,
                "new_stock_level": new_stock_level
            }), 200
        else:
            # This indicates either item not found or insufficient stock
            # We need to explicitly check which
            existing_item = db.inventory.find_one({'store_id': store_id, 'product_id': product_id})
            if existing_item:
                return jsonify({"error": f"Insufficient stock. Current: {existing_item['current_stock']}, Requested: {quantity}"}), 400
            else:
                return jsonify({"error": f"Inventory item not found for Product ID: {product_id} at Store ID: {store_id}"}), 404

    except Exception as e:
        print(f"Error recording sale: {e}")
        return jsonify({"error": f"An unexpected error occurred: {str(e)}"}), 500

@app.route('/inventory/sale_batch', methods=['POST'])
def record_sale_batch():
    """
    Records multiple sales events from a CSV file.
    The CSV file should contain 'store_id', 'product_id', 'quantity' columns.
    """
    try:
        db = get_db()
    except Exception as e:
        return jsonify({"error": f"Database connection error: {e}"}), 500

    if 'file' not in request.files:
        return jsonify({"error": "No file part in the request"}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
    
    if file and file.filename.endswith('.csv'):
        try:
            stream = io.StringIO(file.stream.read().decode("UTF8"))
            df = pd.read_csv(stream)
            df.columns = df.columns.str.strip()

            required_cols = ['store_id', 'product_id', 'quantity']
            if not all(col in df.columns for col in required_cols):
                return jsonify({"error": f"CSV must contain 'store_id', 'product_id', and 'quantity' columns. Missing: {', '.join([col for col in required_cols if col not in df.columns])}"}), 400

            results = []
            batch_size = 50 # Number of operations per bulk write
            delay_between_batches = 0.2 # Small delay for batch operations
            
            # Using a list to hold all operations to be executed in batches
            all_updates_to_perform = []

            for index, row in df.iterrows():
                store_id = str(row['store_id'])
                product_id = str(row['product_id'])
                try:
                    quantity = int(row['quantity'])
                    if quantity <= 0:
                        results.append({"row": index + 1, "status": "failed", "error": "Quantity must be positive.", "store_id": store_id, "product_id": product_id})
                        continue
                except ValueError:
                    results.append({"row": index + 1, "status": "failed", "error": "Invalid quantity format.", "store_id": store_id, "product_id": product_id})
                    continue

                # For sales, we must ensure stock is sufficient before update
                # This check is done in the filter of update_one
                all_updates_to_perform.append({
                    'filter': {'store_id': store_id, 'product_id': product_id, 'current_stock': {'$gte': quantity}},
                    'update': {'$inc': {'current_stock': -quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_sold_quantity': quantity}},
                    'index': index + 1 # Store original row index for results
                })
            
            # Now execute updates in batches
            for i in range(0, len(all_updates_to_perform), batch_size):
                current_batch_updates = all_updates_to_perform[i:i + batch_size]
                
                bulk_write_requests = []
                # Create UpdateOne requests for pymongo's bulk_write
                for update_op in current_batch_updates:
                    bulk_write_requests.append(
                        pymongo.UpdateOne(
                            update_op['filter'],
                            update_op['update']
                        )
                    )
                
                try:
                    if bulk_write_requests: # Only execute if there are operations
                        bulk_result = db.inventory.bulk_write(bulk_write_requests, ordered=False)
                        print(f"  Batch sale ops: {bulk_result.modified_count} modified, {bulk_result.upserted_count} upserted.")
                        
                        # Acknowledge processed rows from this batch
                        for op_info in current_batch_updates:
                            results.append({
                                "row": op_info['index'],
                                "status": "success",
                                "message": "Processed in batch (stock checked)",
                                "store_id": op_info['filter']['store_id'],
                                "product_id": op_info['filter']['product_id']
                            })

                    time.sleep(delay_between_batches) # Add delay after each batch

                except BulkWriteError as bwe:
                    print(f"  Partial success/error in batch sale: {bwe.details}")
                    # Detailed error handling here would involve parsing bwe.details['writeErrors']
                    # For simplicity, we just mark all in batch as potential partial success
                    for op_info in current_batch_updates:
                        results.append({
                            "row": op_info['index'],
                            "status": "partial_success/failure",
                            "message": "See backend logs for details",
                            "store_id": op_info['filter']['store_id'],
                            "product_id": op_info['filter']['product_id']
                        })
                    time.sleep(delay_between_batches)
                except Exception as e:
                    print(f"  Error processing sale batch: {e}")
                    for op_info in current_batch_updates:
                        results.append({
                            "row": op_info['index'],
                            "status": "failed",
                            "error": str(e),
                            "store_id": op_info['filter']['store_id'],
                            "product_id": op_info['filter']['product_id']
                        })
                    time.sleep(delay_between_batches)
            
            return jsonify({"message": "Batch sale processing complete", "results": results}), 200

        except Exception as e:
            return jsonify({"error": f"Error processing CSV file: {str(e)}"}), 500
    else:
        return jsonify({"error": "Invalid file format. Please upload a CSV file."}), 400


@app.route('/inventory/receipt', methods=['POST'])
def record_receipt():
    """
    Records a new stock receipt, incrementing the inventory level for a product
    at a specific store.

    Example Request (JSON Body):
    {
        "store_id": "S1",
        "product_id": "P1",
        "quantity": 50
    }
    Example Response:
    {
        "message": "Receipt recorded successfully",
        "new_stock_level": 200,
        "product_id": "P1",
        "quantity_received": 50,
        "store_id": "S1"
    }
    """
    try:
        db = get_db()
    except Exception as e:
        return jsonify({"error": f"Database connection error: {e}"}), 500

    data = request.get_json()
    store_id = data.get('store_id')
    product_id = data.get('product_id')
    quantity = data.get('quantity')

    # Basic input validation
    if not all([store_id, product_id, quantity is not None]):
        return jsonify({"error": "Missing 'store_id', 'product_id', or 'quantity' in request body."}), 400
    if not isinstance(quantity, int) or quantity <= 0:
        return jsonify({"error": "Quantity must be a positive integer."}), 400

    try:
        # MongoDB upsert: if the document doesn't exist, create it; otherwise, update it.
        # $inc increments the current_stock, $set updates last_updated.
        result = db.inventory.find_one_and_update(
            {'store_id': store_id, 'product_id': product_id},
            {'$inc': {'current_stock': quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_receipt_quantity': quantity}},
            upsert=True, # Create the document if it doesn't exist
            return_document=ReturnDocument.AFTER # Return the document AFTER update
        )

        if result:
            return jsonify({
                "message": "Receipt recorded successfully",
                "product_id": product_id,
                "store_id": store_id,
                "quantity_received": quantity,
                "new_stock_level": result['current_stock']
            }), 200
        else:
            return jsonify({"error": f"Failed to record receipt for Product ID: {product_id} at Store ID: {store_id}"}), 500

    except Exception as e:
        print(f"Error recording receipt: {e}")
        return jsonify({"error": f"An unexpected error occurred: {str(e)}"}), 500

@app.route('/inventory/receipt_batch', methods=['POST'])
def record_receipt_batch():
    """
    Records multiple receipt events from a CSV file.
    The CSV file should contain 'store_id', 'product_id', 'quantity' columns.
    """
    try:
        db = get_db()
    except Exception as e:
        return jsonify({"error": f"Database connection error: {e}"}), 500

    if 'file' not in request.files:
        return jsonify({"error": "No file part in the request"}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
    
    if file and file.filename.endswith('.csv'):
        try:
            stream = io.StringIO(file.stream.read().decode("UTF8"))
            df = pd.read_csv(stream)
            df.columns = df.columns.str.strip()

            required_cols = ['store_id', 'product_id', 'quantity']
            if not all(col in df.columns for col in required_cols):
                return jsonify({"error": f"CSV must contain 'store_id', 'product_id', and 'quantity' columns. Missing: {', '.join([col for col in required_cols if col not in df.columns])}"}), 400

            results = []
            batch_size = 50 # Number of operations per bulk write
            delay_between_batches = 0.2 # Small delay for batch operations
            
            all_updates_to_perform = []

            for index, row in df.iterrows():
                store_id = str(row['store_id'])
                product_id = str(row['product_id'])
                try:
                    quantity = int(row['quantity'])
                    if quantity <= 0:
                        results.append({"row": index + 1, "status": "failed", "error": "Quantity must be positive.", "store_id": store_id, "product_id": product_id})
                        continue
                except ValueError:
                    results.append({"row": index + 1, "status": "failed", "error": "Invalid quantity format.", "store_id": store_id, "product_id": product_id})
                    continue

                all_updates_to_perform.append({
                    'filter': {'store_id': store_id, 'product_id': product_id},
                    'update': {'$inc': {'current_stock': quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_receipt_quantity': quantity}},
                    'upsert': True,
                    'index': index + 1 # Store original row index for results
                })
            
            # Now execute updates in batches
            for i in range(0, len(all_updates_to_perform), batch_size):
                current_batch_updates = all_updates_to_perform[i:i + batch_size]
                
                bulk_write_requests = []
                for update_op in current_batch_updates:
                    bulk_write_requests.append(
                        pymongo.UpdateOne(
                            update_op['filter'],
                            update_op['update'],
                            upsert=update_op['upsert']
                        )
                    )
                
                try:
                    if bulk_write_requests:
                        bulk_result = db.inventory.bulk_write(bulk_write_requests, ordered=False)
                        print(f"  Batch receipt ops: {bulk_result.modified_count} modified, {bulk_result.upserted_count} upserted.")
                        for op_info in current_batch_updates:
                            results.append({
                                "row": op_info['index'],
                                "status": "success",
                                "message": "Processed in batch",
                                "store_id": op_info['filter']['store_id'],
                                "product_id": op_info['filter']['product_id']
                            })
                    time.sleep(delay_between_batches)

                except BulkWriteError as bwe:
                    print(f"  Partial success/error in batch receipt: {bwe.details}")
                    for op_info in current_batch_updates:
                        results.append({
                            "row": op_info['index'],
                            "status": "partial_success/failure",
                            "message": "See backend logs for details",
                            "store_id": op_info['filter']['store_id'],
                            "product_id": op_info['filter']['product_id']
                        })
                    time.sleep(delay_between_batches)
                except Exception as e:
                    print(f"  Error processing receipt batch: {e}")
                    for op_info in current_batch_updates:
                        results.append({
                            "row": op_info['index'],
                            "status": "failed",
                            "error": str(e),
                            "store_id": op_info['filter']['store_id'],
                            "product_id": op_info['filter']['product_id']
                        })
                    time.sleep(delay_between_batches)
            
            return jsonify({"message": "Batch receipt processing complete", "results": results}), 200

        except Exception as e:
            return jsonify({"error": f"Error processing CSV file: {str(e)}"}), 500
    else:
        return jsonify({"error": "Invalid file format. Please upload a CSV file."}), 400


@app.route('/inventory/low_stock_alerts', methods=['GET'])
def get_low_stock_alerts():
    """
    Identifies and returns products across all stores that are projected to run out
    within a specified number of `days_left`, based on their current stock and
    simulated daily sales.

    Query Parameter:
    - `days_left`: Integer, the number of days within which stock is projected to run out (default: 7).
    - `store_id` (optional): Filter alerts for a specific store.
    """
    try:
        db = get_db()
    except Exception as e:
        return jsonify({"error": f"Database connection error: {e}"}), 500

    days_left_str = request.args.get('days_left', '7') # Default to 7 days
    store_filter_id = request.args.get('store_id')

    try:
        days_left_threshold = int(days_left_str)
        if days_left_threshold < 0:
            return jsonify({"error": "days_left must be a non-negative integer."}), 400
    except ValueError:
        return jsonify({"error": "Invalid 'days_left' value. Must be an integer."}), 400

    try:
        critical_stock_items = []
        
        query_filter = {}
        if store_filter_id:
            query_filter['store_id'] = store_filter_id

        # Fetch all inventory items (or filtered by store_id)
        # This operation iterates through the cursor, not bringing all into memory at once
        for item in db.inventory.find(query_filter):
            current_stock = item.get('current_stock', 0)
            daily_demand_sim = item.get('daily_sales_simulation_base', 1) 
            
            days_remaining = float('inf')
            if daily_demand_sim > 0:
                days_remaining = current_stock / daily_demand_sim
            elif current_stock == 0:
                days_remaining = 0.0

            # Only add to alerts if it's projected to run out within the threshold and stock is not already 0.
            if days_remaining <= days_left_threshold and current_stock > 0:
                # Convert MongoDB's _id to string for JSON
                item['_id'] = str(item['_id'])
                # Convert datetime object to string for JSON serialization
                if 'last_updated' in item and isinstance(item['last_updated'], datetime.datetime):
                    item['last_updated'] = item['last_updated'].strftime('%Y-%m-%d %H:%M:%S')
                
                alert_info = {
                    "product_id": item.get('product_id'),
                    "store_id": item.get('store_id'),
                    "current_stock": current_stock,
                    "daily_demand_sim": daily_demand_sim,
                    "days_remaining": round(days_remaining, 2),
                    "alert_reason": f"Projected to run out in {round(days_remaining, 2)} days (within {days_left_threshold} days limit)",
                    "last_updated": item.get('last_updated')
                }
                critical_stock_items.append(alert_info)

        return jsonify(critical_stock_items), 200
    except Exception as e:
        print(f"Error fetching low stock alerts based on days: {e}")
        return jsonify({"error": f"An error occurred while fetching alerts: {str(e)}"}), 500

# --- Running the Flask Application ---
if __name__ == '__main__':
    # IMPORTANT: For initial data load, uncomment the line below ONLY ONCE.
    # After successful load, comment it out again to prevent re-loading on every app start.
    # This will clear existing data in MongoDB and reload everything.
    # load_initial_inventory_data()

    # Ensure MongoDB connection is established before starting the Flask app
    # This will attempt to connect, and if it fails, the app won't start.
    try:
        connect_to_mongodb()
    except Exception as e:
        print(f"Application startup aborted due to MongoDB connection error: {e}")
        exit(1) # Exit if cannot connect to DB

    app.run(debug=True, host='0.0.0.0', port=5000)
