# backend/services/inventory_service.py
import datetime
import os
import json
import time
import pandas as pd
import pymongo # Needed for pymongo.UpdateOne
from pymongo.errors import BulkWriteError, ConnectionFailure
from pymongo import ReturnDocument
import math 

# Paths to your pre-generated NDJSON files (relative to backend/ directory, where data_prep.py placed them)
PRODUCTS_JSON_PATH = 'products.json'
STORES_JSON_PATH = 'stores.json'
INVENTORY_JSON_PATH = 'inventory.json'


def load_initial_inventory_data(db):
    """
    Loads initial data from generated NDJSON files into MongoDB collections.
    This function is intended to be called once for initial data population.
    It includes aggressive rate limiting and basic retry logic.
    
    Args:
        db: The MongoDB database client instance.
    """
    if db is None:
        print("MongoDB database instance not provided. Cannot load initial data.")
        return

    # Adjust paths if data_prep.py places them elsewhere or if this script is run from a different CWD
    current_dir = os.path.dirname(os.path.abspath(__file__))
    backend_dir = os.path.dirname(current_dir) # Go up one level from services to backend
    
    products_file = os.path.join(backend_dir, PRODUCTS_JSON_PATH)
    stores_file = os.path.join(backend_dir, STORES_JSON_PATH)
    inventory_file = os.path.join(backend_dir, INVENTORY_JSON_PATH)

    if not (os.path.exists(products_file) and
            os.path.exists(stores_file) and
            os.path.exists(inventory_file)):
        print(f"Error: One or more NDJSON files not found at expected locations:")
        print(f" - Products: {products_file}")
        print(f" - Stores: {stores_file}")
        print(f" - Inventory: {inventory_file}")
        print("Please ensure they are in the backend/ directory and were generated by data_prep.py.")
        return

    print("Starting initial data load from NDJSON files into MongoDB...")
    
    collections_to_load = {
        'products': products_file,
        'stores': stores_file,
        'inventory': inventory_file,
    }

    # Configuration for rate limiting and retries
    max_batch_size = 100 # Smaller batch size for initial load
    delay_between_batches = 3.0 # Seconds to wait between batch inserts (increased for safety)
    max_retries = 5
    retry_delay_multiplier = 2 # Multiplier for exponential backoff on retry

    print(f"Delay between batches set to: {delay_between_batches} seconds")
    print(f"Max batch size set to: {max_batch_size} operations")

    for collection_name, file_path in collections_to_load.items():
        print(f"\n--- Loading {collection_name} collection from {file_path} ---")
        
        items_to_insert = []
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip(): # Avoid empty lines
                    item = json.loads(line)
                    # Prepare data for MongoDB insertion based on collection type
                    if collection_name == 'inventory':
                        # Ensure 'last_updated' is a datetime object
                        if 'last_updated' in item and isinstance(item['last_updated'], str):
                            try:
                                item['last_updated'] = datetime.datetime.fromisoformat(item['last_updated'])
                            except ValueError:
                                item['last_updated'] = datetime.datetime.now()
                        else:
                            item['last_updated'] = datetime.datetime.now()
                        # Ensure numerical fields are proper types, provide defaults
                        item['current_stock'] = int(item.get('current_stock', 0))
                        item['daily_sales_simulation_base'] = int(item.get('daily_sales_simulation_base', 1))

                    items_to_insert.append(item)
        
        print(f"Read {len(items_to_insert)} items for {collection_name}.")

        if not items_to_insert:
            print(f"No items to load for {collection_name}. Skipping.")
            continue

        current_collection = db[collection_name]
        
        # Drop the collection before inserting to ensure a clean slate on re-run (for development)
        # CAUTION: In production, you would NOT do this for existing data
        print(f"Dropping existing '{collection_name}' collection for clean import...")
        current_collection.drop()

        # Perform bulk inserts with rate limiting
        total_items_processed = 0
        for i in range(0, len(items_to_insert), max_batch_size):
            batch_data = items_to_insert[i:i + max_batch_size]
            
            retries = 0
            while retries < max_retries:
                try:
                    # Insert many documents at once
                    current_collection.insert_many(batch_data, ordered=False) # ordered=False continues on error
                    total_items_processed += len(batch_data)
                    print(f"  Inserted {len(batch_data)} {collection_name} records (Total: {total_items_processed}). Waiting {delay_between_batches}s...")
                    time.sleep(delay_between_batches)
                    break # Batch inserted successfully, break retry loop
                except BulkWriteError as bwe:
                    print(f"  BulkWriteError for {collection_name}: {bwe.details}")
                    # For a hackathon, we might just log and continue on BulkWriteError (e.g., duplicate key)
                    total_items_processed += bwe.details.get('nInserted', 0) # Count successful inserts in batch
                    print(f"  Partial batch inserted. Total: {total_items_processed}. Waiting {delay_between_batches}s...")
                    time.sleep(delay_between_batches)
                    break # Continue even on partial errors for now
                except ConnectionFailure as ce:
                    print(f"  MongoDB ConnectionFailure during insert for {collection_name}: {ce}")
                    retries += 1
                    current_retry_delay = delay_between_batches * (retry_delay_multiplier ** retries)
                    print(f"  Connection error. Retrying in {current_retry_delay}s... (Attempt {retries}/{max_retries})")
                    time.sleep(current_retry_delay)
                except Exception as e:
                    print(f"  Error inserting batch for {collection_name}: {e}")
                    retries += 1
                    current_retry_delay = delay_between_batches * (retry_delay_multiplier ** retries)
                    print(f"  General error. Retrying in {current_retry_delay}s... (Attempt {retries}/{max_retries})")
                    time.sleep(current_retry_delay)

            if retries == max_retries:
                print(f"  Failed to insert batch for {collection_name} after {max_retries} retries. Skipping remaining items in this batch.")

        # Ensure all items processed messages are printed for this collection even if batch_item_count is less than max_batch_size
        if total_items_processed < len(items_to_insert):
            print(f"  Note: Some items might have been skipped for {collection_name} due to errors.")
        
        print(f"Finished loading {collection_name}. Total {total_items_processed} items processed.")

    print("\n--- Initial data load process complete ---")

def get_global_config(db):
    """
    Retrieves the global configuration settings from the database.
    """
    config = db.global_config.find_one({'config_id': 'default_config'})
    if not config:
        # Provide sensible defaults if config is not found (or raise error if strictly required)
        print("Warning: Global config not found. Using default values for transfers.")
        return {
            'config_id': 'default_config',
            'shipping_cost_per_km': 1.0, # Default shipping cost
            'transfer_success_rate': 0.9, # Default success rate
            'current_lead_time_override': 0
        }
    return config

def calculate_haversine_distance(lat1, lon1, lat2, lon2):
    """
    Calculates the Haversine distance between two points on Earth
    given their latitudes and longitudes in degrees.
    Returns distance in kilometers.
    """
    R = 6371  # Earth radius in kilometers

    lat1_rad = math.radians(lat1)
    lon1_rad = math.radians(lon1)
    lat2_rad = math.radians(lat2)
    lon2_rad = math.radians(lon2)

    dlon = lon2_rad - lon1_rad
    dlat = lat2_rad - lat1_rad

    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))

    distance = R * c
    return distance

def get_inventory_item(db, store_id, product_id):
    """
    Retrieves the current stock level and details for a specific product
    at a given store.
    """
    inventory_item = db.inventory.find_one({
        'store_id': store_id,
        'product_id': product_id
    })
    if inventory_item:
        inventory_item['_id'] = str(inventory_item['_id']) # Convert ObjectId to string
        if 'last_updated' in inventory_item and isinstance(inventory_item['last_updated'], datetime.datetime):
            inventory_item['last_updated'] = inventory_item['last_updated'].strftime('%Y-%m-%d %H:%M:%S')
    return inventory_item

def record_sale_transaction(db, store_id, product_id, quantity):
    """
    Records a sale event, decrementing the inventory level.
    Ensures sufficient stock before update.
    Returns the new stock level or raises ValueError.
    """
    result = db.inventory.find_one_and_update(
        {'store_id': store_id, 'product_id': product_id, 'current_stock': {'$gte': quantity}},
        {'$inc': {'current_stock': -quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_sold_quantity': quantity}},
        return_document=ReturnDocument.AFTER
    )
    if result:
        return result['current_stock']
    else:
        existing_item = db.inventory.find_one({'store_id': store_id, 'product_id': product_id})
        if existing_item:
            raise ValueError(f"Insufficient stock. Current: {existing_item['current_stock']}, Requested: {quantity}")
        else:
            raise ValueError(f"Inventory item not found for Product ID: {product_id} at Store ID: {store_id}")

def record_receipt_transaction(db, store_id, product_id, quantity):
    """
    Records a new stock receipt, incrementing the inventory level.
    Creates the entry if it doesn't exist (upsert).
    Returns the new stock level.
    """
    result = db.inventory.find_one_and_update(
        {'store_id': store_id, 'product_id': product_id},
        {'$inc': {'current_stock': quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_receipt_quantity': quantity}},
        upsert=True,
        return_document=ReturnDocument.AFTER
    )
    if result:
        return result['current_stock']
    else:
        # This case should be rare with upsert=True unless a different error occurs
        raise ValueError(f"Failed to record receipt for Product ID: {product_id} at Store ID: {store_id}")


def process_sales_batch_csv(db, csv_file_stream):
    """
    Processes a CSV stream for multiple sales events using bulk write operations.
    """
    df = pd.read_csv(csv_file_stream)
    df.columns = df.columns.str.strip()

    required_cols = ['store_id', 'product_id', 'quantity']
    if not all(col in df.columns for col in required_cols):
        raise ValueError(f"CSV must contain 'store_id', 'product_id', and 'quantity' columns. Missing: {', '.join([col for col in required_cols if col not in df.columns])}")

    results = []
    batch_size = 50
    delay_between_batches = 0.2

    all_updates_to_perform = []

    for index, row in df.iterrows():
        store_id = str(row['store_id'])
        product_id = str(row['product_id'])
        try:
            quantity = int(row['quantity'])
            if quantity <= 0:
                results.append({"row": index + 1, "status": "failed", "error": "Quantity must be positive.", "store_id": store_id, "product_id": product_id})
                continue
        except ValueError:
            results.append({"row": index + 1, "status": "failed", "error": "Invalid quantity format.", "store_id": store_id, "product_id": product_id})
            continue

        all_updates_to_perform.append({
            'filter': {'store_id': store_id, 'product_id': product_id, 'current_stock': {'$gte': quantity}},
            'update': {'$inc': {'current_stock': -quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_sold_quantity': quantity}},
            'index': index + 1 # Store original row index for results
        })
    
    for i in range(0, len(all_updates_to_perform), batch_size):
        current_batch_updates = all_updates_to_perform[i:i + batch_size]
        
        bulk_write_requests = [
            pymongo.UpdateOne(op['filter'], op['update']) for op in current_batch_updates
        ]
        
        try:
            if bulk_write_requests:
                db.inventory.bulk_write(bulk_write_requests, ordered=False)
                for op_info in current_batch_updates:
                    results.append({
                        "row": op_info['index'],
                        "status": "success",
                        "message": "Processed in batch (stock checked)",
                        "store_id": op_info['filter']['store_id'],
                        "product_id": op_info['filter']['product_id']
                    })
            time.sleep(delay_between_batches)

        except BulkWriteError as bwe:
            print(f"  Partial success/error in batch sale: {bwe.details}")
            for op_info in current_batch_updates:
                results.append({
                    "row": op_info['index'],
                    "status": "partial_success/failure",
                    "message": "See backend logs for details",
                    "store_id": op_info['filter']['store_id'],
                    "product_id": op_info['filter']['product_id']
                })
            time.sleep(delay_between_batches)
        except Exception as e:
            print(f"  Error processing sale batch: {e}")
            for op_info in current_batch_updates:
                results.append({
                    "row": op_info['index'],
                    "status": "failed",
                    "error": str(e),
                    "store_id": op_info['filter']['store_id'],
                    "product_id": op_info['filter']['product_id']
                })
            time.sleep(delay_between_batches)
    
    return results

def process_receipts_batch_csv(db, csv_file_stream):
    """
    Processes a CSV stream for multiple receipt events using bulk write operations.
    """
    df = pd.read_csv(csv_file_stream)
    df.columns = df.columns.str.strip()

    required_cols = ['store_id', 'product_id', 'quantity']
    if not all(col in df.columns for col in required_cols):
        raise ValueError(f"CSV must contain 'store_id', 'product_id', and 'quantity' columns. Missing: {', '.join([col for col in required_cols if col not in df.columns])}")

    results = []
    batch_size = 50
    delay_between_batches = 0.2
    
    all_updates_to_perform = []

    for index, row in df.iterrows():
        store_id = str(row['store_id'])
        product_id = str(row['product_id'])
        try:
            quantity = int(row['quantity'])
            if quantity <= 0:
                results.append({"row": index + 1, "status": "failed", "error": "Quantity must be positive.", "store_id": store_id, "product_id": product_id})
                continue
        except ValueError:
            results.append({"row": index + 1, "status": "failed", "error": "Invalid quantity format.", "store_id": store_id, "product_id": product_id})
            continue

        all_updates_to_perform.append({
            'filter': {'store_id': store_id, 'product_id': product_id},
            'update': {'$inc': {'current_stock': quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_receipt_quantity': quantity}},
            'upsert': True,
            'index': index + 1
        })
    
    for i in range(0, len(all_updates_to_perform), batch_size):
        current_batch_updates = all_updates_to_perform[i:i + batch_size]
        
        bulk_write_requests = [
            pymongo.UpdateOne(op['filter'], op['update'], upsert=op['upsert']) for op in current_batch_updates
        ]
        
        try:
            if bulk_write_requests:
                db.inventory.bulk_write(bulk_write_requests, ordered=False)
                for op_info in current_batch_updates:
                    results.append({
                        "row": op_info['index'],
                        "status": "success",
                        "message": "Processed in batch",
                        "store_id": op_info['filter']['store_id'],
                        "product_id": op_info['filter']['product_id']
                    })
            time.sleep(delay_between_batches)

        except BulkWriteError as bwe:
            print(f"  Partial success/error in batch receipt: {bwe.details}")
            for op_info in current_batch_updates:
                results.append({
                    "row": op_info['index'],
                    "status": "partial_success/failure",
                    "message": "See backend logs for details",
                    "store_id": op_info['filter']['store_id'],
                    "product_id": op_info['filter']['product_id']
                })
            time.sleep(delay_between_batches)
        except Exception as e:
            print(f"  Error processing receipt batch: {e}")
            for op_info in current_batch_updates:
                results.append({
                    "row": op_info['index'],
                    "status": "failed",
                    "error": str(e),
                    "store_id": op_info['filter']['store_id'],
                    "product_id": op_info['filter']['product_id']
                })
            time.sleep(delay_between_batches)
    
    return results

def get_low_stock_alerts_data(db, days_left_threshold, store_filter_id=None):
    """
    Identifies and returns products across all stores that are projected to run out
    within a specified number of `days_left`, based on their current stock and
    simulated daily sales. Also incorporates 'min_replenish_time' for advanced alerts.
    Results are sorted by 'days_remaining' in ascending order.
    """
    critical_stock_items = []
    
    query_filter = {}
    if store_filter_id:
        query_filter['store_id'] = store_filter_id

    # Fetch all products once to get their replenishment times
    products_collection = db['products']
    all_products = {}
    for product_doc in products_collection.find({}):
        all_products[product_doc['product_id']] = product_doc.get('min_replenish_time', 0) # Default to 0 if missing

    for item in db.inventory.find(query_filter):
        product_id = item.get('product_id')
        store_id = item.get('store_id')
        current_stock = item.get('current_stock', 0)
        daily_demand_sim = item.get('daily_sales_simulation_base', 1) 
        min_replenish_time = all_products.get(product_id, 0) # Get replenishment time for this product

        days_remaining = float('inf')
        if daily_demand_sim > 0:
            days_remaining = current_stock / daily_demand_sim
        elif current_stock == 0:
            days_remaining = 0.0

        alert_category = "Standard Alert"
        alert_reason = f"Projected to run out in {round(days_remaining, 2)} days (within {days_left_threshold} days limit)"

        if current_stock == 0:
            alert_category = "Critical - Out of Stock"
            alert_reason = "Currently out of stock."
        elif days_remaining <= min_replenish_time and days_remaining > 0:
            alert_category = "Critical - Below Replenishment Lead Time"
            alert_reason = f"Projected to run out in {round(days_remaining, 2)} days, which is less than replenishment time of {min_replenish_time} days."
        elif days_remaining <= days_left_threshold and days_remaining > 0:
            alert_category = "Warning - Approaching Threshold"
            alert_reason = f"Projected to run out in {round(days_remaining, 2)} days (within {days_left_threshold} days limit)."
        else: # If stock is sufficient and not critical, we don't add to alerts list
            continue


        # Only add to alerts list if it matches one of our alert conditions
        if alert_category != "Standard Alert":
            if 'last_updated' in item and isinstance(item['last_updated'], datetime.datetime):
                item['last_updated'] = item['last_updated'].strftime('%Y-%m-%d %H:%M:%S')
            
            alert_info = {
                "product_id": product_id,
                "store_id": store_id,
                "current_stock": current_stock,
                "daily_demand_sim": daily_demand_sim,
                "min_replenish_time": min_replenish_time,
                "days_remaining": round(days_remaining, 2),
                "alert_category": alert_category,
                "alert_reason": alert_reason,
                "last_updated": item.get('last_updated')
            }
            critical_stock_items.append(alert_info)

    # Sort the alerts by 'days_remaining' in ascending order
    critical_stock_items.sort(key=lambda x: x['days_remaining'])

    return critical_stock_items

def get_overstocked_products_data(db, threshold_multiplier, days_for_demand, store_filter_id=None):
    """
    Identifies and returns products across all stores that are considered overstocked.
    An item is overstocked if its current stock is greater than
    (threshold_multiplier * (daily_sales_simulation_base * days_for_demand)).

    Args:
        db: The MongoDB database client instance.
        threshold_multiplier (float): Multiplier for projected demand (e.g., 3.0 for 3x demand).
        days_for_demand (int): Number of days to project demand for.
        store_filter_id (str, optional): Filters alerts for a specific store.
    """
    overstocked_items = []

    query_filter = {}
    if store_filter_id:
        query_filter['store_id'] = store_filter_id

    # Fetch product names for a more descriptive alert
    products_collection = db['products']
    all_products_names = {}
    for product_doc in products_collection.find({}):
        all_products_names[product_doc['product_id']] = product_doc.get('name', 'Unknown Product')

    for item in db.inventory.find(query_filter):
        product_id = item.get('product_id')
        store_id = item.get('store_id')
        current_stock = item.get('current_stock', 0)
        daily_demand_sim = item.get('daily_sales_simulation_base', 1)

        projected_demand = daily_demand_sim * days_for_demand
        
        # Check for overstocked condition
        if projected_demand > 0 and current_stock > (threshold_multiplier * projected_demand):
            # Convert ObjectId to string and datetime to string for JSON serialization
            item['_id'] = str(item['_id'])
            if 'last_updated' in item and isinstance(item['last_updated'], datetime.datetime):
                item['last_updated'] = item['last_updated'].strftime('%Y-%m-%d %H:%M:%S')
            
            overstock_info = {
                "product_id": product_id,
                "product_name": all_products_names.get(product_id, f"Product {product_id}"),
                "store_id": store_id,
                "current_stock": current_stock,
                "daily_demand_sim": daily_demand_sim,
                "projected_demand_for_X_days": round(projected_demand, 2),
                "threshold_multiplier": threshold_multiplier,
                "overstock_ratio": round(current_stock / projected_demand, 2) if projected_demand > 0 else "N/A",
                "alert_reason": (
                    f"Current stock ({current_stock}) is {round(current_stock / projected_demand, 2) if projected_demand > 0 else 'N/A'} times "
                    f"the projected demand of {round(projected_demand, 2)} units over {days_for_demand} days "
                    f"(threshold: {threshold_multiplier}x)."
                ),
                "last_updated": item.get('last_updated')
            }
            overstocked_items.append(overstock_info)

    # Sort overstocked items by overstock_ratio descending (most overstocked first)
    overstocked_items.sort(key=lambda x: x.get('overstock_ratio', 0) if isinstance(x.get('overstock_ratio'), (int, float)) else 0, reverse=True)

    return overstocked_items

def get_demand_forecast_data_ml(db, ml_model, preprocessor, numerical_features, categorical_features, store_id, product_id, num_days=30, **kwargs):
    """
    Generates a demand forecast for a specific product at a given store for future days
    using the loaded machine learning model and preprocessor. Allows for 'what-if' scenario inputs.

    Args:
        db: MongoDB database instance.
        ml_model: The pre-trained machine learning model.
        preprocessor: The pre-trained ColumnTransformer for feature preprocessing.
        numerical_features: List of numerical feature names used during training.
        categorical_features: List of categorical feature names used during training.
        store_id (str): The ID of the store for which to forecast.
        product_id (str): The ID of the product for which to forecast.
        num_days (int): Number of future days to forecast.
        **kwargs: Optional 'what-if' parameters (e.g., 'future_discount', 'future_holiday').
    """
    if ml_model is None or preprocessor is None:
        raise ValueError("ML model or preprocessor not loaded in the backend.")

    inventory_record = db.inventory.find_one(
        {'store_id': store_id, 'product_id': product_id},
        sort=[('last_updated', pymongo.DESCENDING)]
    )
    if not inventory_record:
        raise ValueError(f"Inventory record not found for Product ID: {product_id} at Store ID: {store_id}.")
    
    product_details = db.products.find_one({'product_id': product_id})
    if not product_details:
        raise ValueError(f"Product details not found for Product ID: {product_id}.")

    store_details = db.stores.find_one({'store_id': store_id})
    if not store_details:
        raise ValueError(f"Store details not found for Store ID: {store_id}.")

    last_units_sold = inventory_record.get('last_sold_quantity', 0)
    last_inventory_level = inventory_record.get('current_stock', 0)
    
    avg_price = 10.0
    avg_discount = 0.0

    try:
        product_pricing = db.products.find_one(
            {"product_id": product_id},
            {"price": 1, "discount": 1, "_id": 0}
        )
        if product_pricing:
            if product_pricing.get('price') is not None:
                avg_price = float(product_pricing['price'])
            if product_pricing.get('discount') is not None:
                avg_discount = float(product_pricing['discount'])
        else: 
            try:
                avg_price_doc = db.products.aggregate([{"$group": {"_id": None, "avg_price": {"$avg": "$price"}}}]).next()
                if avg_price_doc and avg_price_doc.get('avg_price') is not None:
                    avg_price = float(avg_price_doc['avg_price'])
            except StopIteration:
                pass
            
            try:
                avg_discount_doc = db.products.aggregate([{"$group": {"_id": None, "avg_discount": {"$avg": "$discount"}}}]).next()
                if avg_discount_doc and avg_discount_doc.get('avg_discount') is not None:
                    avg_discount = float(avg_discount_doc['avg_discount'])
            except StopIteration:
                pass
    except Exception as e:
        print(f"Warning: Could not robustly calculate price/discount for product {product_id}: {e}. Using defaults.")

    units_ordered_future = 0 
    competitor_pricing_default = avg_price * 0.95 
    
    forecast_data = []
    current_date = datetime.date.today()

    # Use kwargs for 'what-if' values, or fall back to defaults
    future_discount_override = kwargs.get('future_discount', avg_discount)
    future_holiday_override = kwargs.get('future_holiday', "No")
    future_weather_override = kwargs.get('future_weather', "Clear") # Default to clear
    future_price_override = kwargs.get('future_price', avg_price)
    future_competitor_pricing_override = kwargs.get('future_competitor_pricing', competitor_pricing_default)


    for i in range(num_days):
        forecast_date = current_date + datetime.timedelta(days=i)
        
        # Apply overrides from what-if scenarios, otherwise use defaults
        weather_condition = future_weather_override
        holiday_promotion = future_holiday_override
        price_for_forecast = future_price_override
        discount_for_forecast = future_discount_override
        competitor_pricing_for_forecast = future_competitor_pricing_override

        month = forecast_date.month
        seasonality = "Spring" if 3 <= month <= 5 else \
                      "Summer" if 6 <= month <= 8 else \
                      "Autumn" if 9 <= month <= 11 else \
                      "Winter" 
        
        future_data_row_dict = {
            'Date': forecast_date, 
            'Store ID': store_id,
            'Product ID': product_id,
            'Category': product_details.get('category', 'Unknown'),
            'Region': store_details.get('region', 'Unknown'),
            'Inventory Level': last_inventory_level,
            'Units Ordered': units_ordered_future,
            'Price': price_for_forecast,
            'Discount': discount_for_forecast,
            'Weather Condition': weather_condition,
            'Holiday/Promotion': holiday_promotion,
            'Competitor Pricing': competitor_pricing_for_forecast,
            'Seasonality': seasonality,
            'Units Sold Lag1': last_units_sold, 
            'Inventory Level Lag1': last_inventory_level 
        }

        future_data_df = pd.DataFrame([future_data_row_dict])
        future_data_df['Date'] = pd.to_datetime(future_data_df['Date'])
        
        future_data_df['Year'] = future_data_df['Date'].dt.year
        future_data_df['Month'] = future_data_df['Date'].dt.month
        future_data_df['Day'] = future_data_df['Date'].dt.day
        future_data_df['DayOfWeek'] = future_data_df['Date'].dt.dayofweek
        future_data_df['WeekOfYear'] = future_data_df['Date'].dt.isocalendar().week.astype(int)

        all_expected_features_for_df = numerical_features + categorical_features

        X_forecast_input = pd.DataFrame(columns=all_expected_features_for_df)
        for col in all_expected_features_for_df:
            if col in future_data_df.columns:
                X_forecast_input[col] = future_data_df[col]
            else:
                if col in numerical_features:
                    X_forecast_input[col] = 0.0 
                elif col in categorical_features:
                    X_forecast_input[col] = 'Unknown'
                else:
                    X_forecast_input[col] = 'NaN' 

        X_forecast_processed = preprocessor.transform(X_forecast_input)

        predicted_demand = ml_model.predict(X_forecast_processed)[0]
        predicted_demand = max(0, round(predicted_demand)) 

        forecast_data.append({
            "date": forecast_date.isoformat(),
            "predicted_demand": predicted_demand,
            "store_id": store_id,
            "product_id": product_id
        })
        
        last_units_sold = predicted_demand 

    return forecast_data

def get_reorder_recommendation(db, ml_model, preprocessor, numerical_features, categorical_features, store_id, product_id):
    """
    Calculates reorder recommendations (quantity, order date, delivery date)
    based on current stock, product lead time, and forecasted demand.
    """
    inventory_record = db.inventory.find_one(
        {'store_id': store_id, 'product_id': product_id}
    )
    if not inventory_record:
        raise ValueError(f"Inventory record not found for Product ID: {product_id} at Store ID: {store_id}.")
    
    product_details = db.products.find_one({'product_id': product_id})
    if not product_details:
        raise ValueError(f"Product details not found for Product ID: {product_id}.")

    current_stock = inventory_record.get('current_stock', 0)
    # Default replenishment time to 7 days if not found in product details
    min_replenish_time = product_details.get('min_replenish_time', 7) 
    
    # Define safety stock days (e.g., 7 days of forecasted demand)
    SAFETY_STOCK_DAYS = 7 
    # Define target inventory days (e.g., maintain 30 days of stock after replenishment)
    TARGET_INVENTORY_DAYS = 30

    # 1. Get demand forecast for lead time + safety stock days
    # We need forecast for `min_replenish_time + SAFETY_STOCK_DAYS` days to calculate total demand until reorder point
    # We'll use the ML-driven forecast.
    total_forecast_days_needed = min_replenish_time + SAFETY_STOCK_DAYS
    
    # Ensure num_days is at least 1 for forecast function
    if total_forecast_days_needed <= 0:
        total_forecast_days_needed = 1 

    forecast_for_lead_time_and_safety = get_demand_forecast_data_ml(
        db, ml_model, preprocessor, numerical_features, categorical_features, 
        store_id, product_id, num_days=total_forecast_days_needed
    )
    
    # Calculate total forecasted demand over the period
    total_forecasted_demand = sum([f['predicted_demand'] for f in forecast_for_lead_time_and_safety])

    # Calculate average daily forecasted demand over the period for safety stock calculation
    average_daily_forecasted_demand = total_forecasted_demand / total_forecast_days_needed if total_forecast_days_needed > 0 else inventory_record.get('daily_sales_simulation_base', 1)

    # Calculate Safety Stock (e.g., 7 days of average forecasted demand)
    safety_stock_units = round(average_daily_forecasted_demand * SAFETY_STOCK_DAYS)

    # Calculate Reorder Point: Demand during lead time + Safety Stock
    # For demand during lead time, use forecast specifically for lead time days
    demand_during_lead_time = sum([f['predicted_demand'] for f in forecast_for_lead_time_and_safety[:min_replenish_time]])
    reorder_point = max(0, round(demand_during_lead_time + safety_stock_units)) # Ensure non-negative

    # Suggested Order Quantity: Quantity to bring stock up to TARGET_INVENTORY_DAYS + Safety Stock
    # Calculate total demand for target inventory days
    forecast_for_target_inventory = get_demand_forecast_data_ml(
        db, ml_model, preprocessor, numerical_features, categorical_features, 
        store_id, product_id, num_days=TARGET_INVENTORY_DAYS
    )
    total_forecasted_demand_target = sum([f['predicted_demand'] for f in forecast_for_target_inventory])

    target_inventory_level = total_forecasted_demand_target + safety_stock_units
    
    suggested_order_quantity = max(0, round(target_inventory_level - current_stock))
    
    # If current stock is already very high (above reorder point and target), suggest 0 or very small
    if current_stock > reorder_point and suggested_order_quantity <= 0:
        suggested_order_quantity = 0

    # Calculate Order Date and Delivery Date
    order_date = datetime.date.today()
    delivery_date = order_date + datetime.timedelta(days=min_replenish_time)

    # Determine if a reorder is currently needed
    reorder_needed = "Yes" if current_stock <= reorder_point and suggested_order_quantity > 0 else "No"
    
    return {
        "store_id": store_id,
        "product_id": product_id,
        "current_stock": current_stock,
        "min_replenish_time_days": min_replenish_time,
        "average_daily_forecasted_demand": round(average_daily_forecasted_demand, 2),
        "safety_stock_units": safety_stock_units,
        "reorder_point_units": reorder_point,
        "reorder_needed": reorder_needed,
        "suggested_order_quantity": suggested_order_quantity,
        "suggested_order_date": order_date.isoformat(),
        "suggested_delivery_date": delivery_date.isoformat(),
        "notes": "Recommendation based on ML demand forecast, lead time, and safety stock. Adjust parameters as needed."
    }

def get_optimal_stocking_data(db, ml_model, preprocessor, numerical_features, categorical_features, store_id, product_id):
    """
    Calculates the optimal stocking level (target inventory) for a given product at a store,
    incorporating forecasted demand, base safety stock, supplier reliability,
    and dynamic lead time adjustments.
    
    Args:
        db: MongoDB database instance.
        ml_model: The pre-trained machine learning model.
        preprocessor: The pre-trained ColumnTransformer for feature preprocessing.
        numerical_features: List of numerical feature names used during training.
        categorical_features: List of categorical feature names used during training.
        store_id (str): The ID of the store.
        product_id (str): The ID of the product.
    
    Returns:
        dict: A dictionary containing optimal stocking recommendations and breakdown.
    Raises:
        ValueError: If product or inventory record is not found, or ML components are missing.
    """
    if ml_model is None or preprocessor is None:
        raise ValueError("ML model or preprocessor not loaded in the backend. Cannot calculate optimal stocking.")

    inventory_record = db.inventory.find_one(
        {'store_id': store_id, 'product_id': product_id}
    )
    if not inventory_record:
        raise ValueError(f"Inventory record not found for Product ID: {product_id} at Store ID: {store_id}.")
    
    product_details = db.products.find_one({'product_id': product_id})
    if not product_details:
        raise ValueError(f"Product details not found for Product ID: {product_id}.")

    # NEW: Fetch global config for lead time override
    global_config = get_global_config(db) # Use the helper function
    current_lead_time_override = global_config.get('current_lead_time_override', 0) # Default to 0

    current_stock = inventory_record.get('current_stock', 0)
    base_safety_stock = product_details.get('base_safety_stock', 10) # Default to 10 if missing
    supplier_category_reliability = product_details.get('supplier_category_reliability', 0.8) # Default to 0.8 if missing
    original_min_replenish_time = product_details.get('min_replenish_time', 7) # Default to 7 if missing

    # --- Calculations for Optimal Stocking ---
    
    # 1. Dynamic Lead Time Adjustments (Feature 5)
    effective_lead_time = original_min_replenish_time + current_lead_time_override
    effective_lead_time = max(1, effective_lead_time) # Ensure lead time is at least 1 day

    # 2. Calculate reliability factor
    reliability_factor = 1.5 * (1 - supplier_category_reliability)
    
    # 3. Calculate adjusted safety stock
    # Integrate with Safety Stock (Feature 5): Adjust safety stock by lead time ratio
    lead_time_ratio = effective_lead_time / original_min_replenish_time if original_min_replenish_time > 0 else 1
    
    # Base safety stock is adjusted by reliability factor, then by lead time ratio
    calculated_safety_stock = round(base_safety_stock * (1 + reliability_factor) * lead_time_ratio)
    calculated_safety_stock = max(0, calculated_safety_stock) # Ensure non-negative

    # 4. Get 30-day demand forecast
    forecast_days_for_optimal_stock = 30 # As per requirement
    
    forecast_data_30_days = get_demand_forecast_data_ml(
        db, ml_model, preprocessor, numerical_features, categorical_features, 
        store_id, product_id, num_days=forecast_days_for_optimal_stock
    )
    
    total_30_day_forecasted_demand = sum([f['predicted_demand'] for f in forecast_data_30_days])

    # 5. Calculate target inventory level
    target_inventory_level = total_30_day_forecasted_demand + calculated_safety_stock
    target_inventory_level = max(0, target_inventory_level)

    return {
        "product_id": product_id,
        "store_id": store_id,
        "current_stock": current_stock,
        "base_safety_stock": base_safety_stock,
        "supplier_category_reliability": supplier_category_reliability,
        "reliability_factor": round(reliability_factor, 2),
        "calculated_safety_stock": calculated_safety_stock,
        "total_30_day_forecasted_demand": total_30_day_forecasted_demand,
        "target_inventory_level": target_inventory_level,
        "min_replenish_time": original_min_replenish_time, # NEW: Original lead time
        "current_lead_time_override": current_lead_time_override, # NEW: Current override
        "effective_lead_time": effective_lead_time, # NEW: Calculated effective lead time
        "optimal_stocking_notes": "Target inventory based on 30-day forecast, adjusted safety stock (considering supplier reliability and dynamic lead time)." # Updated note
    }


def get_remediation_actions(db, ml_model, preprocessor, numerical_features, categorical_features, store_id_filter=None, product_id_filter=None):
    """
    Generates a prioritized list of remediation actions (order, promote, or transfer)
    for understocked or overstocked products based on optimal stocking levels and transfer feasibility,
    including estimated financial impact.

    Args:
        db: MongoDB database instance.
        ml_model: The pre-trained machine learning model.
        preprocessor: The pre-trained ColumnTransformer for feature preprocessing.
        numerical_features: List of numerical feature names used during training.
        categorical_features: List of categorical feature names used during training.
        store_id_filter (str, optional): Filter actions for a specific store.
        product_id_filter (str, optional): Filter actions for a specific product.

    Returns:
        list: A list of dictionaries, each representing a recommended action with financial impact.
    """
    remediation_actions = []
    
    TRANSFER_VIABILITY_THRESHOLD = 70 
    PROMOTION_DISCOUNT_PERCENTAGE = 0.15 # 15% discount for promotion actions

    # Fetch all inventory items (potentially filtered)
    inventory_query = {}
    if store_id_filter:
        inventory_query['store_id'] = store_id_filter
    if product_id_filter:
        inventory_query['product_id'] = product_id_filter

    all_inventory_items_in_scope = list(db.inventory.find(inventory_query))

    # --- Step 1: Identify Understocked and Overstocked Items ---
    understocked_items = []
    overstocked_items = []

    for item in all_inventory_items_in_scope:
        store_id = item['store_id']
        product_id = item['product_id']
        current_stock = item.get('current_stock', 0)

        try:
            optimal_stocking_data = get_optimal_stocking_data(
                db, ml_model, preprocessor, numerical_features, categorical_features,
                store_id, product_id
            )
            optimal_target_level = optimal_stocking_data['target_inventory_level']
            delta = optimal_target_level - current_stock # Positive if under, negative if over

            if delta > 0: # Understock
                understocked_items.append({
                    "store_id": store_id,
                    "product_id": product_id,
                    "current_stock": current_stock,
                    "optimal_target_level": optimal_target_level,
                    "needed_quantity": delta,
                    "priority": "high" if delta > 50 else ("medium" if delta > 10 else "low"),
                    "reason": f"Current stock ({current_stock}) is below optimal target ({optimal_target_level}). Needs {delta} units."
                })
            elif delta < 0: # Overstock
                overstocked_items.append({
                    "store_id": store_id,
                    "product_id": product_id,
                    "current_stock": current_stock,
                    "optimal_target_level": optimal_target_level,
                    "excess_quantity": abs(delta),
                    "priority": "high" if abs(delta) > 100 else ("medium" if abs(delta) > 30 else "low"),
                    "reason": f"Current stock ({current_stock}) is above optimal target ({optimal_target_level}). Excess {abs(delta)} units."
                })
        except ValueError as ve:
            print(f"Skipping optimal stocking calc for {product_id}@{store_id} due to data error: {ve}")
        except Exception as e:
            print(f"Unexpected error in optimal stocking calc for {product_id}@{store_id}: {e}")
            
    # --- Step 2: Prioritize Overstock Remediation (Transfer vs. Promote) ---
    final_remediation_list = [] # Use a new list to build the final actions

    # Add all identified understock actions first (these will be "Order" actions)
    for u_item in understocked_items:
        # If user filtered for a specific store/product, only add if it matches.
        if (not store_id_filter or u_item['store_id'] == store_id_filter) and \
           (not product_id_filter or u_item['product_id'] == product_id_filter):
            
            # Fetch product details for unit_cost (for all actions now, including Order if we want a full P&L)
            product_details = db.products.find_one({'product_id': u_item['product_id']})
            unit_cost = product_details.get('unit_cost', 0)
            product_price = product_details.get('price', 0)

            # For 'Order' action, estimated_net_profit would be positive sales profit * if ordered and sold.
            # Associated_cost would be the unit_cost * quantity.
            # This logic needs to be clear: are we simulating "profit from selling these units if they were available"
            # or "cost of acquiring these units"? Let's assume profit from sales.
            estimated_net_profit = round((product_price - unit_cost) * u_item['needed_quantity'], 2)
            associated_cost = round(unit_cost * u_item['needed_quantity'], 2) # Cost to acquire these units if ordered

            final_remediation_list.append({
                "store_id": u_item['store_id'],
                "product_id": u_item['product_id'],
                "current_stock": u_item['current_stock'],
                "optimal_target_level": u_item['optimal_target_level'],
                "delta": u_item['needed_quantity'],
                "action_type": "Order",
                "suggested_quantity": u_item['needed_quantity'],
                "priority": u_item['priority'],
                "reason": u_item['reason'],
                "estimated_net_profit": estimated_net_profit,
                "associated_cost": associated_cost,
                "impact_notes": "Estimated profit from selling these units if ordered."
            })

    # Process overstock items to find transfers or promote actions
    for overstocked_item in overstocked_items:
        source_store_id = overstocked_item['store_id']
        product_id = overstocked_item['product_id']
        excess_quantity = overstocked_item['excess_quantity']
        
        # If user filtered, ensure this overstocked item is part of the filter scope
        if (store_id_filter and source_store_id != store_id_filter) or \
           (product_id_filter and product_id != product_id_filter):
            continue # Skip if not in filter scope

        # Fetch product details for unit_cost and price
        product_details = db.products.find_one({'product_id': product_id})
        unit_cost = product_details.get('unit_cost', 0)
        product_price = product_details.get('price', 0)

        best_transfer_option = None
        best_transfer_score = -1

        # Search for potential target stores (understocked for the same product)
        potential_targets = [
            u_item for u_item in understocked_items 
            if u_item['product_id'] == product_id and u_item['store_id'] != source_store_id and \
               ((not store_id_filter) or u_item['store_id'] == store_id_filter)
        ]

        for target_item in potential_targets:
            target_store_id = target_item['store_id']
            needed_quantity = target_item['needed_quantity']
            
            qty_to_consider_transfer = min(excess_quantity, needed_quantity)

            if qty_to_consider_transfer <= 0:
                continue

            try:
                feasibility_data = get_transfer_feasibility_score(
                    db, source_store_id, target_store_id, product_id, qty_to_consider_transfer
                )
                
                final_score = feasibility_data['final_feasibility_score']

                if final_score >= TRANSFER_VIABILITY_THRESHOLD and final_score > best_transfer_score:
                    # Calculate financial impact for Transfer
                    # Net Profit = Total Retail Value of transferred goods - Transfer Cost
                    estimated_transfer_retail_value = product_price * qty_to_consider_transfer
                    transfer_cost_actual = feasibility_data['calculated_transfer_cost']
                    estimated_net_profit_transfer = round(estimated_transfer_retail_value - transfer_cost_actual, 2)
                    
                    best_transfer_option = {
                        "action_type": "Transfer",
                        "source_store_id": source_store_id,
                        "target_store_id": target_store_id,
                        "product_id": product_id,
                        "suggested_quantity": qty_to_consider_transfer,
                        "priority": overstocked_item['priority'], # Inherit priority from overstock level
                        "reason": f"Overstocked at {source_store_id}, Understocked at {target_store_id}. Viability: {final_score}/100.",
                        "transfer_details": feasibility_data, # Include all feasibility data
                        "estimated_net_profit": estimated_net_profit_transfer,
                        "associated_cost": transfer_cost_actual,
                        "impact_notes": "Estimated profit after transfer cost and potential sales."
                    }
                    best_transfer_score = final_score
            except ValueError as ve:
                print(f"Skipping transfer feasibility for {product_id} from {source_store_id} to {target_store_id} due to: {ve}")
            except Exception as e:
                print(f"Unexpected error in transfer feasibility calc for {product_id} from {source_store_id} to {target_store_id}: {e}")
        
        if best_transfer_option:
            final_remediation_list.append(best_transfer_option)
        else:
            # If no viable transfer found, default to 'Promote / Reduce' for the overstocked item
            # Calculate financial impact for Promotion
            # Net Profit = ( (Price * (1 - Discount)) - Unit Cost ) * Quantity
            promotional_price = product_price * (1 - PROMOTION_DISCOUNT_PERCENTAGE)
            profit_per_unit_promote = promotional_price - unit_cost
            estimated_net_profit_promote = round(profit_per_unit_promote * excess_quantity, 2)
            
            # Associated cost for promotion is implicitly the revenue reduction from discount
            # Or it could be viewed as the cost of holding the excess inventory if not sold quickly.
            # For this context, let's assume associated_cost is the loss in revenue from discount.
            associated_cost_promote = round((product_price * PROMOTION_DISCOUNT_PERCENTAGE) * excess_quantity, 2)
            
            final_remediation_list.append({
                "store_id": source_store_id,
                "product_id": product_id,
                "current_stock": overstocked_item['current_stock'],
                "optimal_target_level": overstocked_item['optimal_target_level'],
                "delta": -overstocked_item['excess_quantity'], # Keep delta negative for overstock
                "action_type": "Promote / Reduce",
                "suggested_quantity": overstocked_item['excess_quantity'],
                "priority": overstocked_item['priority'],
                "reason": overstocked_item['reason'] + " No viable transfer option found.",
                "estimated_net_profit": estimated_net_profit_promote,
                "associated_cost": associated_cost_promote,
                "impact_notes": f"Estimated profit at {PROMOTION_DISCOUNT_PERCENTAGE*100}% discount. Cost is estimated revenue loss."
            })

    # Sort actions: High priority first, then medium, then low
    priority_order = {"high": 3, "medium": 2, "low": 1}
    final_remediation_list.sort(key=lambda x: priority_order.get(x['priority'], 0), reverse=True)

    return final_remediation_list

def get_transfer_feasibility_score(db, source_store_id, target_store_id, product_id, transfer_quantity):
    """
    Calculates a feasibility score for transferring a product from a source store
    to a target store based on distance, cost, and historical success rates.

    Args:
        db: MongoDB database instance.
        source_store_id (str): ID of the store with overstock (where product ships from).
        target_store_id (str): ID of the store with understock (where product ships to).
        product_id (str): ID of the product to be transferred.
        transfer_quantity (int): The number of units to be transferred.

    Returns:
        dict: Contains the final score, component scores, and breakdown.
    Raises:
        ValueError: If store/product/config data is missing, or quantity is invalid.
    """
    if transfer_quantity <= 0:
        raise ValueError("Transfer quantity must be a positive integer.")

    # Fetch necessary data
    source_store = db.stores.find_one({'store_id': source_store_id})
    target_store = db.stores.find_one({'store_id': target_store_id})
    product_details = db.products.find_one({'product_id': product_id})
    global_config = get_global_config(db) # Use the new helper function

    if not source_store:
        raise ValueError(f"Source store '{source_store_id}' not found.")
    if not target_store:
        raise ValueError(f"Target store '{target_store_id}' not found.")
    if not product_details:
        raise ValueError(f"Product '{product_id}' not found.")
    
    # Check transfer capacity
    if not source_store.get('transfer_capacity', False):
        raise ValueError(f"Source store '{source_store_id}' does not have transfer capacity.")
    if not target_store.get('transfer_capacity', False):
        raise ValueError(f"Target store '{target_store_id}' does not have transfer capacity.")

    # Extract relevant data points
    src_lat = source_store.get('latitude')
    src_lon = source_store.get('longitude')
    tgt_lat = target_store.get('latitude')
    tgt_lon = target_store.get('longitude')
    
    unit_cost = product_details.get('unit_cost')
    product_price = product_details.get('price') # Selling price
    
    shipping_cost_per_km = global_config.get('shipping_cost_per_km')
    historical_transfer_success_rate = global_config.get('transfer_success_rate')

    # Validate essential numerical data
    if any(v is None for v in [src_lat, src_lon, tgt_lat, tgt_lon, unit_cost, product_price, shipping_cost_per_km, historical_transfer_success_rate]):
         raise ValueError("Missing essential geographical, product cost, or global config data for transfer calculation. Ensure all fields are populated.")


    # --- Calculation Steps ---

    # 1. Calculate Haversine Distance (in km)
    haversine_dist = calculate_haversine_distance(src_lat, src_lon, tgt_lat, tgt_lon)
    
    # Approximate road distance (e.g., 30% longer than straight line)
    road_distance_km = haversine_dist * 1.3
    
    # Define a max plausible distance for scoring (adjust based on your typical store network)
    # This acts as a normalization factor. A cross-country transfer would score very low.
    MAX_PLAUSIBLE_DISTANCE_KM = 3000 

    # 2. Calculate Transfer Cost
    transfer_cost = road_distance_km * shipping_cost_per_km * transfer_quantity

    # Define a max acceptable cost relative to product value for scoring
    # This needs to be carefully chosen. E.g., if total product value is $1000, max cost could be $100 (10%)
    # Or, define a flat maximum transfer cost. Let's make it relative to total product value.
    # Max acceptable transfer cost as a percentage of total value being transferred
    MAX_TRANSFER_COST_PERCENT_OF_VALUE = 0.15 # e.g., max 15% of total product value
    max_acceptable_transfer_cost = (product_price * transfer_quantity) * MAX_TRANSFER_COST_PERCENT_OF_VALUE
    # Ensure there's a floor for max_acceptable_transfer_cost to prevent division by zero or too high scores for cheap items
    max_acceptable_transfer_cost = max(max_acceptable_transfer_cost, 10.0) # Minimum $10 acceptable cost

    # 3. Generate Component Scores (0-100)
    # Distance Score: Inverse relationship with distance. Higher score for shorter distance.
    # We'll cap the distance effect to MAX_PLAUSIBLE_DISTANCE_KM. Distances beyond this will score 0.
    distance_score = max(0, 100 - (road_distance_km / MAX_PLAUSIBLE_DISTANCE_KM) * 100)
    
    # Cost Score: Inverse relationship with cost. Higher score for lower cost relative to value.
    # We'll compare transfer_cost to max_acceptable_transfer_cost.
    # If transfer_cost is 0, score is 100. If it's MAX_TRANSFER_COST_PERCENT_OF_VALUE, score is 0.
    cost_ratio = transfer_cost / max_acceptable_transfer_cost
    cost_score = max(0, 100 - (cost_ratio * 100)) # If cost_ratio is 1 (max_acceptable_cost), score is 0.
    
    # Historical Success Rate Score: Directly used. Convert 0-1 to 0-100.
    historical_success_score = historical_transfer_success_rate * 100

    # 4. Calculate Final Score (Weighted Average)
    final_score = (distance_score * 0.40) + (cost_score * 0.30) + (historical_success_score * 0.30)
    final_score = round(final_score) # Round to nearest integer

    # Determine viability category for UI
    viability_category = "red" # <50
    if final_score > 75:
        viability_category = "green"
    elif final_score >= 50: # 50-75
        viability_category = "yellow"

    return {
        "source_store_id": source_store_id,
        "target_store_id": target_store_id,
        "product_id": product_id,
        "transfer_quantity": transfer_quantity,
        "calculated_distance_km": round(road_distance_km, 2),
        "calculated_transfer_cost": round(transfer_cost, 2),
        "distance_score": round(distance_score, 2),
        "cost_score": round(cost_score, 2),
        "historical_success_score": round(historical_success_score, 2),
        "final_feasibility_score": final_score,
        "viability_category": viability_category,
        "notes": "Transfer feasibility based on distance, cost relative to product value, and historical success rates."
    }

