# backend/services/inventory_service.py
import datetime
import os
import json
import time
import pandas as pd
import pymongo # Needed for pymongo.UpdateOne
from pymongo.errors import BulkWriteError, ConnectionFailure
from pymongo import ReturnDocument

# Paths to your pre-generated NDJSON files (relative to backend/ directory, where data_prep.py placed them)
PRODUCTS_JSON_PATH = 'products.json'
STORES_JSON_PATH = 'stores.json'
INVENTORY_JSON_PATH = 'inventory.json'

def load_initial_inventory_data(db):
    """
    Loads initial data from generated NDJSON files into MongoDB collections.
    This function is intended to be called once for initial data population.
    It includes aggressive rate limiting and basic retry logic.
    
    Args:
        db: The MongoDB database client instance.
    """
    if db is None:
        print("MongoDB database instance not provided. Cannot load initial data.")
        return

    # Adjust paths if data_prep.py places them elsewhere or if this script is run from a different CWD
    current_dir = os.path.dirname(os.path.abspath(__file__))
    backend_dir = os.path.dirname(current_dir) # Go up one level from services to backend
    
    products_file = os.path.join(backend_dir, PRODUCTS_JSON_PATH)
    stores_file = os.path.join(backend_dir, STORES_JSON_PATH)
    inventory_file = os.path.join(backend_dir, INVENTORY_JSON_PATH)

    if not (os.path.exists(products_file) and
            os.path.exists(stores_file) and
            os.path.exists(inventory_file)):
        print(f"Error: One or more NDJSON files not found at expected locations:")
        print(f" - Products: {products_file}")
        print(f" - Stores: {stores_file}")
        print(f" - Inventory: {inventory_file}")
        print("Please ensure they are in the backend/ directory and were generated by data_prep.py.")
        return

    print("Starting initial data load from NDJSON files into MongoDB...")
    
    collections_to_load = {
        'products': products_file,
        'stores': stores_file,
        'inventory': inventory_file,
    }

    # Configuration for rate limiting and retries
    max_batch_size = 100 # Smaller batch size for initial load
    delay_between_batches = 3.0 # Seconds to wait between batch inserts (increased for safety)
    max_retries = 5
    retry_delay_multiplier = 2 # Multiplier for exponential backoff on retry

    print(f"Delay between batches set to: {delay_between_batches} seconds")
    print(f"Max batch size set to: {max_batch_size} operations")

    for collection_name, file_path in collections_to_load.items():
        print(f"\n--- Loading {collection_name} collection from {file_path} ---")
        
        items_to_insert = []
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip(): # Avoid empty lines
                    item = json.loads(line)
                    # Prepare data for MongoDB insertion based on collection type
                    if collection_name == 'inventory':
                        # Ensure 'last_updated' is a datetime object
                        if 'last_updated' in item and isinstance(item['last_updated'], str):
                            try:
                                item['last_updated'] = datetime.datetime.fromisoformat(item['last_updated'])
                            except ValueError:
                                item['last_updated'] = datetime.datetime.now()
                        else:
                            item['last_updated'] = datetime.datetime.now()
                        # Ensure numerical fields are proper types, provide defaults
                        item['current_stock'] = int(item.get('current_stock', 0))
                        item['daily_sales_simulation_base'] = int(item.get('daily_sales_simulation_base', 1))

                    items_to_insert.append(item)
        
        print(f"Read {len(items_to_insert)} items for {collection_name}.")

        if not items_to_insert:
            print(f"No items to load for {collection_name}. Skipping.")
            continue

        current_collection = db[collection_name]
        
        # Drop the collection before inserting to ensure a clean slate on re-run (for development)
        # CAUTION: In production, you would NOT do this for existing data
        print(f"Dropping existing '{collection_name}' collection for clean import...")
        current_collection.drop()

        # Perform bulk inserts with rate limiting
        total_items_processed = 0
        for i in range(0, len(items_to_insert), max_batch_size):
            batch_data = items_to_insert[i:i + max_batch_size]
            
            retries = 0
            while retries < max_retries:
                try:
                    # Insert many documents at once
                    current_collection.insert_many(batch_data, ordered=False) # ordered=False continues on error
                    total_items_processed += len(batch_data)
                    print(f"  Inserted {len(batch_data)} {collection_name} records (Total: {total_items_processed}). Waiting {delay_between_batches}s...")
                    time.sleep(delay_between_batches)
                    break # Batch inserted successfully, break retry loop
                except BulkWriteError as bwe:
                    print(f"  BulkWriteError for {collection_name}: {bwe.details}")
                    # For a hackathon, we might just log and continue on BulkWriteError (e.g., duplicate key)
                    total_items_processed += bwe.details.get('nInserted', 0) # Count successful inserts in batch
                    print(f"  Partial batch inserted. Total: {total_items_processed}. Waiting {delay_between_batches}s...")
                    time.sleep(delay_between_batches)
                    break # Continue even on partial errors for now
                except ConnectionFailure as ce:
                    print(f"  MongoDB ConnectionFailure during insert for {collection_name}: {ce}")
                    retries += 1
                    current_retry_delay = delay_between_batches * (retry_delay_multiplier ** retries)
                    print(f"  Connection error. Retrying in {current_retry_delay}s... (Attempt {retries}/{max_retries})")
                    time.sleep(current_retry_delay)
                except Exception as e:
                    print(f"  Error inserting batch for {collection_name}: {e}")
                    retries += 1
                    current_retry_delay = delay_between_batches * (retry_delay_multiplier ** retries)
                    print(f"  General error. Retrying in {current_retry_delay}s... (Attempt {retries}/{max_retries})")
                    time.sleep(current_retry_delay)

            if retries == max_retries:
                print(f"  Failed to insert batch for {collection_name} after {max_retries} retries. Skipping remaining items in this batch.")

        # Ensure all items processed messages are printed for this collection even if batch_item_count is less than max_batch_size
        if total_items_processed < len(items_to_insert):
            print(f"  Note: Some items might have been skipped for {collection_name} due to errors.")
        
        print(f"Finished loading {collection_name}. Total {total_items_processed} items processed.")

    print("\n--- Initial data load process complete ---")

def get_inventory_item(db, store_id, product_id):
    """
    Retrieves the current stock level and details for a specific product
    at a given store.
    """
    inventory_item = db.inventory.find_one({
        'store_id': store_id,
        'product_id': product_id
    })
    if inventory_item:
        inventory_item['_id'] = str(inventory_item['_id']) # Convert ObjectId to string
        if 'last_updated' in inventory_item and isinstance(inventory_item['last_updated'], datetime.datetime):
            inventory_item['last_updated'] = inventory_item['last_updated'].strftime('%Y-%m-%d %H:%M:%S')
    return inventory_item

def record_sale_transaction(db, store_id, product_id, quantity):
    """
    Records a sale event, decrementing the inventory level.
    Ensures sufficient stock before update.
    Returns the new stock level or raises ValueError.
    """
    result = db.inventory.find_one_and_update(
        {'store_id': store_id, 'product_id': product_id, 'current_stock': {'$gte': quantity}},
        {'$inc': {'current_stock': -quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_sold_quantity': quantity}},
        return_document=ReturnDocument.AFTER
    )
    if result:
        return result['current_stock']
    else:
        existing_item = db.inventory.find_one({'store_id': store_id, 'product_id': product_id})
        if existing_item:
            raise ValueError(f"Insufficient stock. Current: {existing_item['current_stock']}, Requested: {quantity}")
        else:
            raise ValueError(f"Inventory item not found for Product ID: {product_id} at Store ID: {store_id}")

def record_receipt_transaction(db, store_id, product_id, quantity):
    """
    Records a new stock receipt, incrementing the inventory level.
    Creates the entry if it doesn't exist (upsert).
    Returns the new stock level.
    """
    result = db.inventory.find_one_and_update(
        {'store_id': store_id, 'product_id': product_id},
        {'$inc': {'current_stock': quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_receipt_quantity': quantity}},
        upsert=True,
        return_document=ReturnDocument.AFTER
    )
    if result:
        return result['current_stock']
    else:
        # This case should be rare with upsert=True unless a different error occurs
        raise ValueError(f"Failed to record receipt for Product ID: {product_id} at Store ID: {store_id}")


def process_sales_batch_csv(db, csv_file_stream):
    """
    Processes a CSV stream for multiple sales events using bulk write operations.
    """
    df = pd.read_csv(csv_file_stream)
    df.columns = df.columns.str.strip()

    required_cols = ['store_id', 'product_id', 'quantity']
    if not all(col in df.columns for col in required_cols):
        raise ValueError(f"CSV must contain 'store_id', 'product_id', and 'quantity' columns. Missing: {', '.join([col for col in required_cols if col not in df.columns])}")

    results = []
    batch_size = 50
    delay_between_batches = 0.2

    all_updates_to_perform = []

    for index, row in df.iterrows():
        store_id = str(row['store_id'])
        product_id = str(row['product_id'])
        try:
            quantity = int(row['quantity'])
            if quantity <= 0:
                results.append({"row": index + 1, "status": "failed", "error": "Quantity must be positive.", "store_id": store_id, "product_id": product_id})
                continue
        except ValueError:
            results.append({"row": index + 1, "status": "failed", "error": "Invalid quantity format.", "store_id": store_id, "product_id": product_id})
            continue

        all_updates_to_perform.append({
            'filter': {'store_id': store_id, 'product_id': product_id, 'current_stock': {'$gte': quantity}},
            'update': {'$inc': {'current_stock': -quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_sold_quantity': quantity}},
            'index': index + 1 # Store original row index for results
        })
    
    for i in range(0, len(all_updates_to_perform), batch_size):
        current_batch_updates = all_updates_to_perform[i:i + batch_size]
        
        bulk_write_requests = [
            pymongo.UpdateOne(op['filter'], op['update']) for op in current_batch_updates
        ]
        
        try:
            if bulk_write_requests:
                db.inventory.bulk_write(bulk_write_requests, ordered=False)
                for op_info in current_batch_updates:
                    results.append({
                        "row": op_info['index'],
                        "status": "success",
                        "message": "Processed in batch (stock checked)",
                        "store_id": op_info['filter']['store_id'],
                        "product_id": op_info['filter']['product_id']
                    })
            time.sleep(delay_between_batches)

        except BulkWriteError as bwe:
            print(f"  Partial success/error in batch sale: {bwe.details}")
            for op_info in current_batch_updates:
                results.append({
                    "row": op_info['index'],
                    "status": "partial_success/failure",
                    "message": "See backend logs for details",
                    "store_id": op_info['filter']['store_id'],
                    "product_id": op_info['filter']['product_id']
                })
            time.sleep(delay_between_batches)
        except Exception as e:
            print(f"  Error processing sale batch: {e}")
            for op_info in current_batch_updates:
                results.append({
                    "row": op_info['index'],
                    "status": "failed",
                    "error": str(e),
                    "store_id": op_info['filter']['store_id'],
                    "product_id": op_info['filter']['product_id']
                })
            time.sleep(delay_between_batches)
    
    return results

def process_receipts_batch_csv(db, csv_file_stream):
    """
    Processes a CSV stream for multiple receipt events using bulk write operations.
    """
    df = pd.read_csv(csv_file_stream)
    df.columns = df.columns.str.strip()

    required_cols = ['store_id', 'product_id', 'quantity']
    if not all(col in df.columns for col in required_cols):
        raise ValueError(f"CSV must contain 'store_id', 'product_id', and 'quantity' columns. Missing: {', '.join([col for col in required_cols if col not in df.columns])}")

    results = []
    batch_size = 50
    delay_between_batches = 0.2
    
    all_updates_to_perform = []

    for index, row in df.iterrows():
        store_id = str(row['store_id'])
        product_id = str(row['product_id'])
        try:
            quantity = int(row['quantity'])
            if quantity <= 0:
                results.append({"row": index + 1, "status": "failed", "error": "Quantity must be positive.", "store_id": store_id, "product_id": product_id})
                continue
        except ValueError:
            results.append({"row": index + 1, "status": "failed", "error": "Invalid quantity format.", "store_id": store_id, "product_id": product_id})
            continue

        all_updates_to_perform.append({
            'filter': {'store_id': store_id, 'product_id': product_id},
            'update': {'$inc': {'current_stock': quantity}, '$set': {'last_updated': datetime.datetime.now(), 'last_receipt_quantity': quantity}},
            'upsert': True,
            'index': index + 1
        })
    
    for i in range(0, len(all_updates_to_perform), batch_size):
        current_batch_updates = all_updates_to_perform[i:i + batch_size]
        
        bulk_write_requests = [
            pymongo.UpdateOne(op['filter'], op['update'], upsert=op['upsert']) for op in current_batch_updates
        ]
        
        try:
            if bulk_write_requests:
                db.inventory.bulk_write(bulk_write_requests, ordered=False)
                for op_info in current_batch_updates:
                    results.append({
                        "row": op_info['index'],
                        "status": "success",
                        "message": "Processed in batch",
                        "store_id": op_info['filter']['store_id'],
                        "product_id": op_info['filter']['product_id']
                    })
            time.sleep(delay_between_batches)

        except BulkWriteError as bwe:
            print(f"  Partial success/error in batch receipt: {bwe.details}")
            for op_info in current_batch_updates:
                results.append({
                    "row": op_info['index'],
                    "status": "partial_success/failure",
                    "message": "See backend logs for details",
                    "store_id": op_info['filter']['store_id'],
                    "product_id": op_info['filter']['product_id']
                })
            time.sleep(delay_between_batches)
        except Exception as e:
            print(f"  Error processing receipt batch: {e}")
            for op_info in current_batch_updates:
                results.append({
                    "row": op_info['index'],
                    "status": "failed",
                    "error": str(e),
                    "store_id": op_info['filter']['store_id'],
                    "product_id": op_info['filter']['product_id']
                })
            time.sleep(delay_between_batches)
    
    return results

def get_low_stock_alerts_data(db, days_left_threshold, store_filter_id=None):
    """
    Identifies and returns products across all stores that are projected to run out
    within a specified number of `days_left`, based on their current stock and
    simulated daily sales. Also incorporates 'min_replenish_time' for advanced alerts.
    Results are sorted by 'days_remaining' in ascending order.
    """
    critical_stock_items = []
    
    query_filter = {}
    if store_filter_id:
        query_filter['store_id'] = store_filter_id

    # Fetch all products once to get their replenishment times
    products_collection = db['products']
    all_products = {}
    for product_doc in products_collection.find({}):
        all_products[product_doc['product_id']] = product_doc.get('min_replenish_time', 0) # Default to 0 if missing

    for item in db.inventory.find(query_filter):
        product_id = item.get('product_id')
        store_id = item.get('store_id')
        current_stock = item.get('current_stock', 0)
        daily_demand_sim = item.get('daily_sales_simulation_base', 1) 
        min_replenish_time = all_products.get(product_id, 0) # Get replenishment time for this product

        days_remaining = float('inf')
        if daily_demand_sim > 0:
            days_remaining = current_stock / daily_demand_sim
        elif current_stock == 0:
            days_remaining = 0.0

        alert_category = "Standard Alert"
        alert_reason = f"Projected to run out in {round(days_remaining, 2)} days (within {days_left_threshold} days limit)"

        if current_stock == 0:
            alert_category = "Critical - Out of Stock"
            alert_reason = "Currently out of stock."
        elif days_remaining <= min_replenish_time and days_remaining > 0:
            alert_category = "Critical - Below Replenishment Lead Time"
            alert_reason = f"Projected to run out in {round(days_remaining, 2)} days, which is less than replenishment time of {min_replenish_time} days."
        elif days_remaining <= days_left_threshold and days_remaining > 0:
            alert_category = "Warning - Approaching Threshold"
            alert_reason = f"Projected to run out in {round(days_remaining, 2)} days (within {days_left_threshold} days limit)."
        else: # If stock is sufficient and not critical, we don't add to alerts list
            continue


        # Only add to alerts list if it matches one of our alert conditions
        if alert_category != "Standard Alert":
            if 'last_updated' in item and isinstance(item['last_updated'], datetime.datetime):
                item['last_updated'] = item['last_updated'].strftime('%Y-%m-%d %H:%M:%S')
            
            alert_info = {
                "product_id": product_id,
                "store_id": store_id,
                "current_stock": current_stock,
                "daily_demand_sim": daily_demand_sim,
                "min_replenish_time": min_replenish_time,
                "days_remaining": round(days_remaining, 2),
                "alert_category": alert_category,
                "alert_reason": alert_reason,
                "last_updated": item.get('last_updated')
            }
            critical_stock_items.append(alert_info)

    # Sort the alerts by 'days_remaining' in ascending order
    critical_stock_items.sort(key=lambda x: x['days_remaining'])

    return critical_stock_items

def get_overstocked_products_data(db, threshold_multiplier, days_for_demand, store_filter_id=None):
    """
    Identifies and returns products across all stores that are considered overstocked.
    An item is overstocked if its current stock is greater than
    (threshold_multiplier * (daily_sales_simulation_base * days_for_demand)).

    Args:
        db: The MongoDB database client instance.
        threshold_multiplier (float): Multiplier for projected demand (e.g., 3.0 for 3x demand).
        days_for_demand (int): Number of days to project demand for.
        store_filter_id (str, optional): Filters alerts for a specific store.
    """
    overstocked_items = []

    query_filter = {}
    if store_filter_id:
        query_filter['store_id'] = store_filter_id

    # Fetch product names for a more descriptive alert
    products_collection = db['products']
    all_products_names = {}
    for product_doc in products_collection.find({}):
        all_products_names[product_doc['product_id']] = product_doc.get('name', 'Unknown Product')

    for item in db.inventory.find(query_filter):
        product_id = item.get('product_id')
        store_id = item.get('store_id')
        current_stock = item.get('current_stock', 0)
        daily_demand_sim = item.get('daily_sales_simulation_base', 1)

        projected_demand = daily_demand_sim * days_for_demand
        
        # Check for overstocked condition
        if projected_demand > 0 and current_stock > (threshold_multiplier * projected_demand):
            # Convert ObjectId to string and datetime to string for JSON serialization
            item['_id'] = str(item['_id'])
            if 'last_updated' in item and isinstance(item['last_updated'], datetime.datetime):
                item['last_updated'] = item['last_updated'].strftime('%Y-%m-%d %H:%M:%S')
            
            overstock_info = {
                "product_id": product_id,
                "product_name": all_products_names.get(product_id, f"Product {product_id}"),
                "store_id": store_id,
                "current_stock": current_stock,
                "daily_demand_sim": daily_demand_sim,
                "projected_demand_for_X_days": round(projected_demand, 2),
                "threshold_multiplier": threshold_multiplier,
                "overstock_ratio": round(current_stock / projected_demand, 2) if projected_demand > 0 else "N/A",
                "alert_reason": (
                    f"Current stock ({current_stock}) is {round(current_stock / projected_demand, 2) if projected_demand > 0 else 'N/A'} times "
                    f"the projected demand of {round(projected_demand, 2)} units over {days_for_demand} days "
                    f"(threshold: {threshold_multiplier}x)."
                ),
                "last_updated": item.get('last_updated')
            }
            overstocked_items.append(overstock_info)

    # Sort overstocked items by overstock_ratio descending (most overstocked first)
    overstocked_items.sort(key=lambda x: x.get('overstock_ratio', 0) if isinstance(x.get('overstock_ratio'), (int, float)) else 0, reverse=True)

    return overstocked_items
